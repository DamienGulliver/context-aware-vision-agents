{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, Owlv2Processor, Owlv2ForObjectDetection\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib.colors import to_rgb\n",
    "import os\n",
    "import anthropic\n",
    "import networkx as nx\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import clear_output\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "TEST_VIDEO_PATH = os.getenv(\"TEST_VIDEO_PATH\") # replace with a path to a .mov file in your google drive\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "class SceneReasoningSystem:\n",
    "    def __init__(self, api_key, history_length=5, db_path='scene_reasoning.db'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.processor, self.model = self.load_object_detector()\n",
    "        self.captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\", device=self.device)\n",
    "        self.history = deque(maxlen=history_length)\n",
    "        self.claude_client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "\n",
    "    def init_database(self):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS reasoning_logs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "            frame_number INTEGER,\n",
    "            scene_description TEXT,\n",
    "            inference TEXT\n",
    "        )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def log_reasoning(self, frame_number, scene_description, inference):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            # Print out types and first 100 characters of each parameter\n",
    "            print(f\"frame_number type: {type(frame_number)}, value: {frame_number}\")\n",
    "            print(f\"scene_description type: {type(scene_description)}, first 100 chars: {scene_description[:100]}\")\n",
    "            print(f\"inference type: {type(inference)}, first 100 chars: {inference[:100]}\")\n",
    "            \n",
    "            # Ensure all parameters are strings\n",
    "            frame_number = str(frame_number)\n",
    "            scene_description = str(scene_description)\n",
    "            inference = str(inference)\n",
    "            \n",
    "            cursor.execute('''\n",
    "            INSERT INTO reasoning_logs (timestamp, frame_number, scene_description, inference)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            ''', (\n",
    "                datetime.now().isoformat(),\n",
    "                frame_number,\n",
    "                scene_description,\n",
    "                inference\n",
    "            ))\n",
    "            conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def load_object_detector(self):\n",
    "        processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "        model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(self.device)\n",
    "        return processor, model\n",
    "\n",
    "    def detect_objects(self, image, texts=[['a man']]):\n",
    "        inputs = self.processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        target_sizes = torch.tensor([image.size[::-1]]).to(self.device)\n",
    "        results = self.processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n",
    "\n",
    "        objects = []\n",
    "        for score, label, box in zip(results[0]['scores'], results[0]['labels'], results[0]['boxes']):\n",
    "            box = [round(i, 2) for i in box.tolist()]\n",
    "            objects.append({\n",
    "                'label': texts[0][label],\n",
    "                'score': score.item(),\n",
    "                'box': box\n",
    "            })\n",
    "        return objects\n",
    "\n",
    "    def estimate_depth(self, box, image_size):\n",
    "        width, height = image_size\n",
    "        box_width = box[2] - box[0]\n",
    "        box_height = box[3] - box[1]\n",
    "        box_area = box_width * box_height\n",
    "        image_area = width * height\n",
    "        \n",
    "        size_factor = box_area / image_area\n",
    "        position_factor = box[3] / height\n",
    "        \n",
    "        depth = 1 - (0.1 * size_factor + 0.9 * position_factor)\n",
    "        return depth\n",
    "\n",
    "    def calculate_spatial_relations(self, objects, image_size):\n",
    "        relations = []\n",
    "        depths = [self.estimate_depth(obj['box'], image_size) for obj in objects]\n",
    "        \n",
    "        for i, (obj1, depth1) in enumerate(zip(objects, depths)):\n",
    "            for j, (obj2, depth2) in enumerate(zip(objects[i+1:], depths[i+1:]), start=i+1):\n",
    "                distance = euclidean(self.get_center(obj1['box']), self.get_center(obj2['box']))\n",
    "                relations.append({\n",
    "                    'obj1': obj1['label'],\n",
    "                    'obj2': obj2['label'],\n",
    "                    'distance': distance,\n",
    "                    'relative_position': self.get_relative_position(obj1['box'], obj2['box']),\n",
    "                    'depth_difference': depth1 - depth2\n",
    "                })\n",
    "        return relations, depths\n",
    "\n",
    "    def get_center(self, box):\n",
    "        return ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "\n",
    "    def get_relative_position(self, box1, box2):\n",
    "        center1 = self.get_center(box1)\n",
    "        center2 = self.get_center(box2)\n",
    "        dx = center2[0] - center1[0]\n",
    "        dy = center2[1] - center1[1]\n",
    "        if abs(dx) > abs(dy):\n",
    "            return \"right\" if dx > 0 else \"left\"\n",
    "        else:\n",
    "            return \"below\" if dy > 0 else \"above\"\n",
    "\n",
    "    def crop_and_caption_objects(self, image, objects, depths):\n",
    "        cropped_images = []\n",
    "        for obj, depth in zip(objects, depths):\n",
    "            if depth < 0.4:\n",
    "                spatial_tag = \"Foreground\"\n",
    "            elif depth > 0.7:\n",
    "                spatial_tag = \"Background\"\n",
    "            else:\n",
    "                spatial_tag = \"Midground\"\n",
    "            \n",
    "            cropped_img = image.crop(obj['box'])\n",
    "            caption = self.captioner(cropped_img)[0]['generated_text']\n",
    "            cropped_images.append({\n",
    "                'label': obj['label'],\n",
    "                'spatial_tag': spatial_tag,\n",
    "                'caption': caption\n",
    "            })\n",
    "        return cropped_images\n",
    "\n",
    "    def analyze_frame(self, frame, frame_number):\n",
    "        objects = self.detect_objects(frame)\n",
    "        spatial_relations, depths = self.calculate_spatial_relations(objects, frame.size)\n",
    "        cropped_images = self.crop_and_caption_objects(frame, objects, depths)\n",
    "        scene_graph = self.generate_scene_graph(cropped_images)\n",
    "        print(scene_graph)\n",
    "        \n",
    "        # Generate hierarchical redundant arguments\n",
    "        hierarchical_args = self.generate_hierarchical_args(scene_graph)\n",
    "        \n",
    "        # Perform second round of object detection with hierarchical arguments\n",
    "        detailed_objects = self.detect_objects(frame, texts=[hierarchical_args])\n",
    "        detailed_spatial_relations, detailed_depths = self.calculate_spatial_relations(detailed_objects, frame.size)\n",
    "        \n",
    "        self.history.append({\n",
    "            'objects': objects,\n",
    "            'relations': spatial_relations,\n",
    "            'captions': cropped_images,\n",
    "            'scene_graph': scene_graph,\n",
    "            'detailed_objects': detailed_objects,\n",
    "            'detailed_relations': detailed_spatial_relations\n",
    "        })\n",
    "        return self.infer_actions_and_motives(frame_number)\n",
    "\n",
    "    def generate_scene_graph(self, cropped_images):\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        for idx, img_data in enumerate(cropped_images):\n",
    "            if img_data['label'] == 'a man':\n",
    "                man_id = f\"man_{idx}\"\n",
    "                G.add_node(man_id, label=\"man\")\n",
    "                \n",
    "                # Add redundant nodes\n",
    "                redundant_labels = [\"human\", \"person\", \"individual\"]\n",
    "                for label in redundant_labels:\n",
    "                    G.add_node(f\"{label}_{idx}\", label=label)\n",
    "                    G.add_edge(f\"{label}_{idx}\", man_id, relation=\"is\")\n",
    "                \n",
    "                # Generate body parts and objects\n",
    "                body_parts = [\"head\", \"eye\", \"arm\", \"hand\", \"leg\", \"foot\"]\n",
    "                objects = [\"book\", \"phone\", \"glass\", \"chair\"]\n",
    "                \n",
    "                for part in body_parts:\n",
    "                    part_id = f\"{part}_{idx}\"\n",
    "                    G.add_node(part_id, label=part)\n",
    "                    G.add_edge(man_id, part_id, relation=\"has\")\n",
    "                    \n",
    "                    # Add redundant nodes for body parts\n",
    "                    redundant_parts = [f\"{man_id}'s {part}\", f\"human {part}\", f\"person's {part}\"]\n",
    "                    for redundant in redundant_parts:\n",
    "                        G.add_node(f\"{redundant}_{idx}\", label=redundant)\n",
    "                        G.add_edge(f\"{redundant}_{idx}\", part_id, relation=\"is\")\n",
    "                \n",
    "                # Add potential objects (based on caption)\n",
    "                caption_lower = img_data['caption'].lower()\n",
    "                for obj in objects:\n",
    "                    if obj in caption_lower:\n",
    "                        obj_id = f\"{obj}_{idx}\"\n",
    "                        G.add_node(obj_id, label=obj)\n",
    "                        G.add_edge(man_id, obj_id, relation=\"interacting_with\")\n",
    "                        \n",
    "                        # Add redundant nodes for objects\n",
    "                        redundant_objs = [f\"{man_id}'s {obj}\", f\"human's {obj}\", f\"person's {obj}\"]\n",
    "                        for redundant in redundant_objs:\n",
    "                            G.add_node(f\"{redundant}_{idx}\", label=redundant)\n",
    "                            G.add_edge(f\"{redundant}_{idx}\", obj_id, relation=\"is\")\n",
    "        \n",
    "        return nx.node_link_data(G)  # Convert to JSON-serializable format\n",
    "\n",
    "    def generate_hierarchical_args(self, scene_graph):\n",
    "        hierarchical_args = []\n",
    "        G = nx.node_link_graph(scene_graph)\n",
    "        for node, data in G.nodes(data=True):\n",
    "            hierarchical_args.append(data['label'])\n",
    "        return list(set(hierarchical_args))  # Remove duplicates\n",
    "\n",
    "    def infer_actions_and_motives(self, frame_number):\n",
    "        scene_description = self.generate_scene_description()\n",
    "        scene_graph = self.history[-1]['scene_graph']\n",
    "        detailed_objects = self.history[-1]['detailed_objects']\n",
    "        detailed_relations = self.history[-1]['detailed_relations']\n",
    "        \n",
    "        # Convert scene graph to a string representation\n",
    "        scene_graph_str = json.dumps(scene_graph, indent=2)\n",
    "        \n",
    "        # Generate detailed spatial information string\n",
    "        detailed_spatial_info = self.generate_detailed_spatial_info(detailed_objects, detailed_relations)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following scene description, scene graph, and detailed spatial information, infer the actions and motives of the actors in the scene. Provide a detailed analysis of what might be happening. Consider alternative explanations to what is given in the scene descriptions. Use the spatial position of eyes to infer the direction of gaze:\n",
    "\n",
    "Scene Description:\n",
    "{scene_description}\n",
    "\n",
    "Scene Graph:\n",
    "{scene_graph_str}\n",
    "\n",
    "Detailed Spatial Information:\n",
    "{detailed_spatial_info}\n",
    "\n",
    "Analysis:\"\"\"\n",
    "        print(prompt)\n",
    "        response = self.claude_client.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        inference = response.content\n",
    "        \n",
    "        # Log the reasoning trace\n",
    "        self.log_reasoning(frame_number, scene_description + \"\\n\\nScene Graph:\\n\" + scene_graph_str + \"\\n\\nDetailed Spatial Information:\\n\" + detailed_spatial_info, inference)\n",
    "        \n",
    "        return inference\n",
    "\n",
    "    def generate_scene_description(self):\n",
    "        description = \"Scene Description:\\n\\n\"\n",
    "        for i, frame_data in enumerate(self.history):\n",
    "            description += f\"Frame {i+1}:\\n\"\n",
    "            for obj in frame_data['objects']:\n",
    "                description += f\"- Detected {obj['label']} at {obj['box']}\\n\"\n",
    "            for relation in frame_data['relations']:\n",
    "                description += f\"- {relation['obj1']} is {relation['relative_position']} {relation['obj2']} (distance: {relation['distance']:.2f}, depth difference: {relation['depth_difference']:.2f})\\n\"\n",
    "            for caption_data in frame_data['captions']:\n",
    "                description += f\"- {caption_data['label']} ({caption_data['spatial_tag']}): {caption_data['caption']}\\n\"\n",
    "            description += \"\\n\"\n",
    "        return description\n",
    "\n",
    "    def generate_detailed_spatial_info(self, detailed_objects, detailed_relations):\n",
    "        info = \"Detailed Spatial Information:\\n\\n\"\n",
    "        for obj in detailed_objects:\n",
    "            info += f\"- Detected {obj['label']} at {obj['box']} with confidence {obj['score']:.2f}\\n\"\n",
    "        for relation in detailed_relations:\n",
    "            info += f\"- {relation['obj1']} is {relation['relative_position']} {relation['obj2']} (distance: {relation['distance']:.2f}, depth difference: {relation['depth_difference']:.2f})\\n\"\n",
    "        return info\n",
    "    \n",
    "    def visualize_frame(self, frame, objects, depths):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.imshow(frame)\n",
    "        ax.axis('off')\n",
    "\n",
    "        for obj, depth in zip(objects, depths):\n",
    "            box = obj['box']\n",
    "            color = plt.cm.viridis(depth)[:3]\n",
    "            rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], \n",
    "                                 fill=False, edgecolor=color, linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            spatial_tag = \"Foreground\" if depth < 0.4 else \"Background\" if depth > 0.7 else \"Midground\"\n",
    "            ax.text(box[0], box[1], f\"{obj['label']}: {obj['score']:.2f}\\n{spatial_tag}\", \n",
    "                    bbox=dict(facecolor='white', alpha=0.8), fontsize=10, color=color)\n",
    "\n",
    "        plt.title(\"Detected Objects with Spatial Tags\")\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_scene_graph(self, scene_graph):\n",
    "        G = nx.node_link_graph(scene_graph)\n",
    "        pos = nx.spring_layout(G)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=8, font_weight='bold')\n",
    "        edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "        plt.title(\"Scene Graph\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def process_single_image(self, image_path):\n",
    "        if os.path.exists(image_path):\n",
    "            image = Image.open(image_path)\n",
    "            inference = self.analyze_frame(image, frame_number=1)\n",
    "            print(inference)\n",
    "            \n",
    "            self.visualize_frame(image, self.history[-1]['objects'], self.history[-1]['relations'][1])\n",
    "            self.visualize_scene_graph(self.history[-1]['scene_graph'])\n",
    "        else:\n",
    "            print(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "    def process_video_stream(self, video_path, frame_interval=30):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video file\")\n",
    "            return\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        frame_count = 0\n",
    "        processed_frames = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count % frame_interval == 0:\n",
    "                processed_frames += 1\n",
    "                print(f\"Processing frame {frame_count}/{total_frames}\")\n",
    "\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "                inference = self.analyze_frame(pil_image, frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} Analysis:\")\n",
    "                print(inference)\n",
    "\n",
    "                self.visualize_frame(pil_image, self.history[-1]['objects'], self.history[-1]['relations'][1])\n",
    "                self.visualize_scene_graph(self.history[-1]['scene_graph'])\n",
    "\n",
    "                cv2_imshow(frame)\n",
    "                clear_output(wait=True)\n",
    "            else:\n",
    "                print(f\"Skipping frame {frame_count}/{total_frames}\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        processed_fps = processed_frames / total_time\n",
    "        print(f\"Processed {processed_frames} frames out of {total_frames} total frames\")\n",
    "        print(f\"Processing time: {total_time:.2f} seconds\")\n",
    "        print(f\"Processed frames per second: {processed_fps:.2f}\")\n",
    "        print(f\"Original video FPS: {fps}\")\n",
    "        print(f\"Frame interval: {frame_interval}\")\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Usage example\n",
    "api_key = ANTHROPIC_API_KEY\n",
    "system = SceneReasoningSystem(api_key)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to your .mov file in Google Drive\n",
    "video_path = TEST_VIDEO_PATH\n",
    "\n",
    "# Process the video\n",
    "system.process_video_stream(video_path, frame_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def view_database(db_path='scene_reasoning.db', limit=10):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # Query to select all rows from the reasoning_logs table\n",
    "    query = f\"SELECT * FROM reasoning_logs LIMIT {limit}\"\n",
    "    \n",
    "    # Use pandas to read the SQL query into a DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "    \n",
    "    # Return the DataFrame in case you want to do more with it\n",
    "    return df\n",
    "\n",
    "df = view_database()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
